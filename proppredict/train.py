import os
import argparse
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import roc_auc_score, average_precision_score, f1_score
from scipy.stats import sem, t
import subprocess
import pickle

# === CONFIG (defaults, can be overridden via CLI) ===
default_config = {
    "metric": "auc",
    "num_epochs": 30,
    "base_dir": "coadd_collins_cv_train_output",
    "dataset_path": "your_dataset.csv",
    "external_folds": 5,
    "internal_folds": 5,
    "smiles_col": "SMILES",
    "target_col": "ACTIVITY",
    "seed": 42
}


def mean_ci(scores, confidence=0.95):
    mean = np.mean(scores)
    stderr = sem(scores)
    ci_range = t.ppf((1 + confidence) / 2., len(scores) - 1) * stderr
    return mean, (mean - ci_range, mean + ci_range)

def safe_run_chemprop_train(train_path, val_path, test_path, save_dir, config):
    command = [
        "chemprop_train",
        "--data_path", train_path,
        "--separate_val_path", val_path,
        "--separate_test_path", test_path,
        "--save_preds",
        "--dataset_type", "classification",
        "--smiles_column", config["smiles_col"],
        "--target_columns", config["target_col"],
        "--save_dir", save_dir,
        "--num_folds", "1",
        "--metric", config["metric"],
        "--epochs", str(config["num_epochs"])
    ]
    subprocess.run(command, check=True)

def run_internal_cv(train_val_df, ext_dir, ext_fold_idx, config):
    skf_internal = StratifiedKFold(n_splits=config["internal_folds"], shuffle=True, random_state=ext_fold_idx)
    best_score = -np.inf
    best_model_dir = None

    for int_fold_idx, (int_train_idx, int_val_idx) in enumerate(skf_internal.split(train_val_df, train_val_df[config["target_col"]])):
        model_dir = os.path.join(ext_dir, f"int_{int_fold_idx}", "model")
        val_scores_path = os.path.join(model_dir, "val_scores.json")
        if os.path.exists(val_scores_path):
            print(f"â­ï¸  int_{int_fold_idx} already trained â€” skipping to val score loading")
            try:
                scores = pd.read_json(val_scores_path, typ='series')
                if scores[config["metric"]] > best_score:
                    best_score = scores[config["metric"]]
                    best_model_dir = model_dir
            except Exception as e:
                print(f"âŒ Failed to read existing score for int_{int_fold_idx}: {e}")
            continue
        int_dir = os.path.join(ext_dir, f"int_{int_fold_idx}")
        os.makedirs(int_dir, exist_ok=True)

        int_train_df = train_val_df.iloc[int_train_idx].reset_index(drop=True)
        int_val_df = train_val_df.iloc[int_val_idx].reset_index(drop=True)

        train_path = os.path.join(int_dir, "train.csv")
        val_path = os.path.join(int_dir, "val.csv")
        int_train_df.to_csv(train_path, index=False)
        int_val_df.to_csv(val_path, index=False)

        model_dir = os.path.join(int_dir, "model")
        
        safe_run_chemprop_train(train_path,
                                val_path,
                                # test_path, #TODO: rewrite to incorporate test_path 
                                model_dir,
                                config) 

        
        if not os.path.exists(val_scores_path):
            print(f"âš ï¸ val_scores.json not found for int_{int_fold_idx} â€” trying manual prediction")

            val_preds_path = os.path.join(model_dir, "val_preds.csv")

            subprocess.run([
                "chemprop_predict",
                "--test_path", val_path,
                "--checkpoint_path", os.path.join(model_dir, "fold_0", "model_0", "model.pt"),
                "--preds_path", val_preds_path,
                "--smiles_column", config["smiles_col"]
            ], check=True)

            val_df = pd.read_csv(val_path)
            preds = pd.read_csv(val_preds_path)

            # if "prediction" in preds.columns:
            y_true = val_df[config["target_col"]]
            # y_score = preds.get("prediction", preds[config["target_col"]])
            y_score = pd.to_numeric(preds.get("prediction", preds[config["target_col"]]), errors="coerce")
            if y_score.isnull().any():
                print("âš ï¸ Warning: Some predicted scores could not be converted to floats.")
                y_score = y_score.fillna(0.0)  # or drop rows

            try:
       
                
                # if config["metric"] == "auc":
                roc_auc = roc_auc_score(y_true, y_score)
                # elif config["metric"] == "f1":
                y_pred = (y_score >= 0.5).astype(int)
                f1 = f1_score(y_true, y_pred)
                # elif config["metric"] == "auc_pr":
                auc_pr = average_precision_score(y_true, y_score)
                # else:
                    # raise ValueError(f"Unsupported metric: {config['metric']}")
                with open(val_scores_path, "w") as f:
                    f.write(f'{{"auc": {roc_auc:.4f}, "f1": {f1:.4f}, "auc_pr": {auc_pr:.4f}}}')
                print(f"âœ… Manually computed and saved val scores: AUC={roc_auc:.4f}, F1={f1:.4f}, PR={auc_pr:.4f}")


            except Exception as e:
                print(f"âŒ Failed to compute score manually: {e}")
                continue
            # else:
            #     print("âŒ Prediction column missing from manual prediction output")
            #     continue

        try:
            scores = pd.read_json(val_scores_path, typ='series')
            if scores[config["metric"]] > best_score:
                best_score = scores[config["metric"]]
                best_model_dir = model_dir
        except Exception as e:
            print(f"âŒ Failed to read AUC for int_{int_fold_idx}: {e}")

    if best_model_dir is None:
        raise RuntimeError(f"âŒ No valid internal models found for {ext_dir} â€” check internal CV logs.")

    return best_model_dir


def refit_final_model(train_val_df, test_df, ext_dir, config, best_model_dir):
    final_dir = os.path.join(ext_dir, "final")
    os.makedirs(final_dir, exist_ok=True)

    # Combine the train and val splits from the best internal model
    best_int_dir = os.path.dirname(best_model_dir)
    best_train_df = pd.read_csv(os.path.join(best_int_dir, "train.csv"))
    best_val_df = pd.read_csv(os.path.join(best_int_dir, "val.csv"))
    combined_train_df = pd.concat([best_train_df, best_val_df], ignore_index=True)

    final_train_csv = os.path.join(final_dir, "train.csv")
    final_test_csv = os.path.join(final_dir, "test.csv")
    final_model_dir = os.path.join(final_dir, "model")

    combined_train_df.to_csv(final_train_csv, index=False)
    test_df.to_csv(final_test_csv, index=False)

    # Use checkpoint_path for warm-starting from best internal model
    # subprocess.run([
    #     "chemprop_train",
    #     "--data_path", final_train_csv,
    #     "--separate_val_path", final_test_csv,  # treat test set as validation
    #     "--save_preds",
    #     "--dataset_type", "classification",
    #     "--smiles_column", config["smiles_col"],
    #     "--target_columns", config["target_col"],
    #     "--save_dir", final_model_dir,
    #     "--num_folds", "1",
    #     "--metric", config["metric"],
    #     "--checkpoint_path", os.path.join(best_model_dir, "fold_0", "model_0", "model.pt")
    # ], check=True)
    safe_run_chemprop_train(
                            # train_path,
                            # val_path,
                            # test_path, #TODO: rewrite to incorporate test_path, update var names
                            # model_dir,
                            config) 
    

    return os.path.join(final_model_dir, "val_preds.csv")



def evaluate_predictions(preds_csv, config):
    test_preds = pd.read_csv(preds_csv)
    y_true = test_preds[config["target_col"]]
    # y_score = test_preds[config["target_col"]]
    if "prediction" not in test_preds.columns and config["target_col"] not in test_preds.columns:
        raise ValueError("âŒ No prediction column found in test predictions.")
    # y_score = test_preds.get("prediction", test_preds[config["target_col"]])
    y_score = pd.to_numeric(test_preds.get("prediction", test_preds[config["target_col"]]), errors="coerce")
    if y_score.isnull().any():
        print("âš ï¸ Warning: Some predicted scores could not be converted to floats.")
        y_score = y_score.fillna(0.0)  # or drop rows
    y_pred = (y_score >= 0.5).astype(int)

    if config["metric"] == "auc":
        metric_val = roc_auc_score(y_true, y_score)
    elif config["metric"] == "f1":
        metric_val = f1_score(y_true, y_pred)
    elif config["metric"] == "auc_pr":
        metric_val = average_precision_score(y_true, y_score)
    else:
        raise ValueError(f"Unsupported metric: {config['metric']}")

    return {config["metric"]: metric_val}


def run_kfold_cv(config):
    df = pd.read_csv(config["dataset_path"])
    skf = StratifiedKFold(n_splits=config["external_folds"], shuffle=True, random_state=config["seed"])

    use_holdout = config.get("holdout_test_path") is not None

    if use_holdout:
        print("ğŸ§ª Using a held-out test set for evaluation across all folds.")
        test_df = pd.read_csv(config["holdout_test_path"])
    else:
        print("âš ï¸ No held-out test set provided â€” each fold will be evaluated on its own validation split.")

    fold_metrics = []

    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df, df[config["target_col"]])):
        print(f"ğŸ” Fold {fold_idx + 1}/{config['external_folds']}")

        train_df = df.iloc[train_idx].reset_index(drop=True)
        val_df = df.iloc[val_idx].reset_index(drop=True)

        fold_dir = os.path.join(config["base_dir"], f"kfold_{fold_idx}")
        os.makedirs(fold_dir, exist_ok=True)

        train_path = os.path.join(fold_dir, "train.csv")
        val_path = os.path.join(fold_dir, "val.csv")
        test_path = os.path.join(fold_dir, "test.csv")

        # Save train and val normally
        train_df.to_csv(train_path, index=False)
        val_df.to_csv(val_path, index=False)

        if use_holdout:
            test_df.to_csv(test_path, index=False)
        else:
            # Reuse validation set for testing if no holdout provided
            val_df.to_csv(test_path, index=False)

        model_dir = os.path.join(fold_dir, "model")

        # Use patched Chemprop call
        safe_run_chemprop_train(train_path, val_path, test_path, model_dir, config)

        preds_path = os.path.join(model_dir, "val_preds.csv")
        preds = pd.read_csv(preds_path)

        y_true = test_df[config["target_col"]] if use_holdout else val_df[config["target_col"]]
        y_score = preds.get("prediction", preds[config["target_col"]])
        y_pred = (y_score >= 0.5).astype(int)

        metrics = {
            "auc_roc": roc_auc_score(y_true, y_score),
            "auc_pr": average_precision_score(y_true, y_score),
            "f1": f1_score(y_true, y_pred)
        }

        fold_metrics.append(metrics)
        print(f"âœ… Fold {fold_idx} metrics: {metrics}")

    metrics_df = pd.DataFrame(fold_metrics)
    metrics_df.to_csv(os.path.join(config["base_dir"], "kfold_metrics.csv"), index=False)

    print("\nğŸ“Š Final K-Fold CV Summary:")
    for metric in metrics_df.columns:
        mean = metrics_df[metric].mean()
        stderr = sem(metrics_df[metric])
        ci = t.ppf((1 + 0.95) / 2., len(metrics_df[metric]) - 1) * stderr
        print(f"{metric.upper()}: {mean:.3f} Â± {ci:.3f}")


def run_nested_cv(config):
    df = pd.read_csv(config["dataset_path"])
    skf = StratifiedKFold(n_splits=config["external_folds"], shuffle=True, random_state=config["seed"])
    splits = list(skf.split(df, df[config["target_col"]]))

    external_metrics = []

    for ext_fold_idx, (train_val_idx, test_idx) in enumerate(splits):
        ext_dir = os.path.join(config["base_dir"], f"ext_{ext_fold_idx}")
        preds_csv = os.path.join(ext_dir, "final", "model", "test_preds.csv")

        if os.path.exists(preds_csv):
            print(f"âœ… ext_{ext_fold_idx} already complete â€” skipping")
            continue

        train_val_df = df.iloc[train_val_idx].reset_index(drop=True)
        test_df = df.iloc[test_idx].reset_index(drop=True)

        print(f"\nğŸ” Running internal CV for ext_{ext_fold_idx}...")
        best_model_dir = run_internal_cv(train_val_df, ext_dir, ext_fold_idx, config)

        print(f"ğŸ”„ Re-training best model for ext_{ext_fold_idx}...")
        preds_csv = refit_final_model(train_val_df, test_df, ext_dir, config, best_model_dir)

        print(f"ğŸ“Š Evaluating predictions for ext_{ext_fold_idx}...")
        metrics = evaluate_predictions(preds_csv, config)
        print(f"  âœ… Test Metrics: {metrics}")
        external_metrics.append(metrics)

    if external_metrics:
        external_df = pd.DataFrame(external_metrics)
        external_df.to_csv(os.path.join(config["base_dir"], "external_metrics.csv"), index=False)
        print(f"ğŸ’¾ Saved detailed test metrics to {os.path.join(config['base_dir'], 'external_metrics.csv')}")
        print("\nğŸ“Š Final Summary Across External Folds:")
        for metric in external_df.columns:
            mean, (ci_low, ci_high) = mean_ci(external_df[metric])
            print(f"{metric.upper()}: {mean:.3f} (95% CI: {ci_low:.3f}â€“{ci_high:.3f})")
    else:
        print("\nâš ï¸ No folds were successfully evaluated.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run nested CV with Chemprop")
    parser.add_argument("--metric", type=str, default=default_config["metric"], choices=["auc", "f1", "auc_pr"], help="Metric to optimize and evaluate")
    parser.add_argument("--num_epochs", type=int, default=default_config["num_epochs"])
    parser.add_argument("--base_dir", type=str, default=default_config["base_dir"])
    parser.add_argument("--dataset_path", type=str, default=default_config["dataset_path"])
    parser.add_argument("--external_folds", type=int, default=default_config["external_folds"])
    parser.add_argument("--internal_folds", type=int, default=default_config["internal_folds"])
    parser.add_argument("--smiles_col", type=str, default=default_config["smiles_col"])
    parser.add_argument("--target_col", type=str, default=default_config["target_col"])
    parser.add_argument("--seed", type=int, default=default_config["seed"])
    parser.add_argument("--holdout_test_path", type=str, default=None,
                    help="Optional path to a held-out test set. If provided, all folds will evaluate on this.")
    parser.add_argument("--cv_mode", type=str, default="nested", choices=["nested", "kfold"],
    help="Choose between 'nested' or 'kfold' cross-validation strategies"
)


    args = parser.parse_args()
    config = vars(args)

    if config["cv_mode"] == "kfold":
        run_kfold_cv(config)
    else:
        run_nested_cv(config)

